{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cbde11-79e4-4865-869e-11f8349cc640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6427fcfb-df79-46ac-abe1-9e3295a3990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## File Imports\n",
    "dataset_path = './Data/'\n",
    "\n",
    "train_labels = pd.read_csv(os.path.join(dataset_path, 'train_labels.csv'))\n",
    "test_data = pd.read_csv(os.path.join(dataset_path, 'sample_submission.csv'))\n",
    "\n",
    "train_path = dataset_path + 'train/'\n",
    "test_path = dataset_path + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b6acb0-63d3-40b2-ba51-3c75458db512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    130908\n",
      "1     89117\n",
      "Name: count, dtype: int64\n",
      "Null:  id       0\n",
      "label    0\n",
      "dtype: int64\n",
      "Missing images: 0\n"
     ]
    }
   ],
   "source": [
    "## Data Analysis\n",
    "print(train_labels['label'].value_counts())\n",
    "print(\"Null: \",train_labels.isnull().sum())\n",
    "\n",
    "missing_images = []\n",
    "for image_name in train_labels['id']:\n",
    "    image_path = os.path.join(dataset_path, 'train', f'{image_name}.tif')\n",
    "    if not os.path.exists(image_path):\n",
    "        missing_images.append(image_name)\n",
    "\n",
    "print(f\"Missing images: {len(missing_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab7e713-75a6-4ddb-926d-83ae1f336264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define preprocessing transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor and normalize to [0, 1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Optionally normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb7482-6fb4-4973-8268-2f705a4d514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Apply preprocessing to a single image\n",
    "image_path = os.path.join(dataset_path, 'train', '000b35e7c39c6cb32224dcb3fe4c48acf34f0252.tif')\n",
    "image = Image.open(image_path)\n",
    "image_preprocessed = preprocess(image)\n",
    "\n",
    "# Convert back to a displayable format and show\n",
    "image_show = transforms.ToPILImage()(image_preprocessed)\n",
    "plt.imshow(image_show)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81ca3f5-773b-437d-88c7-dfb5163e9b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 176020\n",
      "Validation set size: 44005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_set, val_set = train_test_split(train_labels, test_size=0.2, stratify=train_labels['label'], random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fbc427-4dd5-4849-85eb-fc999f898e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx, 0]\n",
    "        label = self.dataframe.iloc[idx, 1]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.tif\")\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CancerDataset(train_set, os.path.join(dataset_path, 'train'), transform=preprocess)\n",
    "val_dataset = CancerDataset(val_set, os.path.join(dataset_path, 'train'), transform=preprocess)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8746bee1-1452-435c-95fe-48f2ba14fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Edward\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to match the number of classes (binary classification)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Binary classification\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "243a3462-0212-4dc6-8d79-8eae0bdf6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid activation with binary cross-entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8eddfe-9dc5-489b-9eea-ed3a69256aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.2456\n",
      "Validation Loss: 0.1829\n",
      "Epoch [2/25], Loss: 0.1785\n",
      "Validation Loss: 0.2050\n",
      "Epoch [3/25], Loss: 0.1472\n",
      "Validation Loss: 0.1445\n",
      "Epoch [4/25], Loss: 0.1251\n",
      "Validation Loss: 0.1185\n",
      "Epoch [5/25], Loss: 0.1060\n",
      "Validation Loss: 0.1218\n",
      "Epoch [6/25], Loss: 0.0887\n",
      "Validation Loss: 0.1405\n",
      "Epoch [7/25], Loss: 0.0732\n",
      "Validation Loss: 0.1300\n",
      "Epoch [8/25], Loss: 0.0594\n",
      "Validation Loss: 0.1322\n",
      "Epoch [9/25], Loss: 0.0496\n",
      "Validation Loss: 0.1440\n",
      "Epoch [10/25], Loss: 0.0428\n",
      "Validation Loss: 0.1723\n",
      "Epoch [11/25], Loss: 0.0371\n",
      "Validation Loss: 0.1444\n",
      "Epoch [12/25], Loss: 0.0326\n",
      "Validation Loss: 0.1942\n",
      "Epoch [13/25], Loss: 0.0304\n",
      "Validation Loss: 0.1806\n",
      "Epoch [14/25], Loss: 0.0275\n",
      "Validation Loss: 0.1809\n",
      "Epoch [15/25], Loss: 0.0253\n",
      "Validation Loss: 0.1814\n",
      "Epoch [16/25], Loss: 0.0234\n",
      "Validation Loss: 0.1793\n",
      "Epoch [17/25], Loss: 0.0212\n",
      "Validation Loss: 0.1747\n",
      "Epoch [18/25], Loss: 0.0207\n",
      "Validation Loss: 0.1746\n",
      "Epoch [19/25], Loss: 0.0192\n",
      "Validation Loss: 0.1789\n",
      "Epoch [20/25], Loss: 0.0184\n",
      "Validation Loss: 0.1855\n",
      "Epoch [21/25], Loss: 0.0169\n",
      "Validation Loss: 0.1990\n",
      "Epoch [22/25], Loss: 0.0161\n",
      "Validation Loss: 0.1990\n",
      "Epoch [23/25], Loss: 0.0155\n",
      "Validation Loss: 0.1793\n",
      "Epoch [24/25], Loss: 0.0159\n",
      "Validation Loss: 0.1763\n",
      "Epoch [25/25], Loss: 0.0142\n",
      "Validation Loss: 0.2087\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device, dtype=torch.float)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics for each epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device, dtype=torch.float)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c50eb766-499c-4814-97b9-8c7f0ecaa154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9570\n",
      "AUC-ROC: 0.9550\n",
      "Confusion Matrix:\n",
      "[[25275   907]\n",
      " [  987 16836]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device, dtype=torch.float)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs.squeeze()).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert predictions to binary labels (0 or 1)\n",
    "all_preds = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "auc = roc_auc_score(all_labels, all_preds)\n",
    "print(f'AUC-ROC: {auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(f'Confusion Matrix:\\n{cm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aef2071-2cc9-45a0-a6bc-6dc5c7eb921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\AppData\\Local\\Temp\\ipykernel_6860\\3400278482.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('cancer_detection_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'cancer_detection_model.pth')\n",
    "\n",
    "# Load the model (for later use)\n",
    "model.load_state_dict(torch.load('cancer_detection_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e440d7c8-a196-4f3c-a3a1-6d41dbaf9698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9570\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425cc310-6073-4352-bd95-297501c89688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9489\n",
      "Recall: 0.9446\n",
      "F1-Score: 0.9467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc3ec81-720d-4ade-b232-99a6b3f62b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9550\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_roc = roc_auc_score(all_labels, all_preds)\n",
    "print(f'AUC-ROC: {auc_roc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e650af-1188-4f1e-b01e-05a7629963d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[25275   907]\n",
      " [  987 16836]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(f'Confusion Matrix:\\n{cm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8458b5-0f2f-4cb0-ad0e-5f878b00c54c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assume you're using a scikit-learn compatible model or can wrap a PyTorch model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(model, X_train, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCross-Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assume you're using a scikit-learn compatible model or can wrap a PyTorch model\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy: {scores.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866261a4-231f-4d99-9143-fd42f5c2c705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
